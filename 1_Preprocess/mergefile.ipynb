{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e368adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Các thư viện Machine Learning\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Các thư viện Deep Learning\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hoàn tất ---\n",
      "Đã xử lý xong 15 bài.\n",
      "File mới đã được lưu tại: data/train03_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def process_data_json(input_path, output_path):\n",
    "    try:\n",
    "        # Kiểm tra xem file có tồn tại không\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"Lỗi: Không tìm thấy file tại đường dẫn: {input_path}\")\n",
    "            return\n",
    "\n",
    "        # 1. Đọc dữ liệu từ đường dẫn data/train02.json\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Kiểm tra cấu trúc dữ liệu\n",
    "        if not isinstance(data, list):\n",
    "            print(\"Lỗi: Cấu trúc JSON phải là một danh sách các bài viết.\")\n",
    "            return\n",
    "\n",
    "        processed_data = []\n",
    "        \n",
    "        # 2. Lấy 15 bài đầu tiên (hoặc ít hơn nếu file không đủ 15 bài)\n",
    "        # Đánh số ID tự động từ 31 đến 45\n",
    "        for i, item in enumerate(data[:15], start=31):\n",
    "            new_item = {\n",
    "                \"id\": i,\n",
    "                \"text\": item.get(\"text\", \"\"),\n",
    "                \"label\": item.get(\"label\", [])\n",
    "            }\n",
    "            processed_data.append(new_item)\n",
    "\n",
    "        # 3. Xuất file kết quả\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"--- Hoàn tất ---\")\n",
    "        print(f\"Đã xử lý xong {len(processed_data)} bài.\")\n",
    "        print(f\"File mới đã được lưu tại: {output_path}\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Lỗi: File JSON không đúng định dạng.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Có lỗi xảy ra: {e}\")\n",
    "\n",
    "# --- THỰC THI ---\n",
    "# Đường dẫn bạn cung cấp\n",
    "input_file = 'data/train03.json' \n",
    "output_file = 'data/train03_cleaned.json'\n",
    "\n",
    "process_data_json(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca038170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- THÀNH CÔNG ---\n",
      "Đã gộp xong 59 bài vào file: train.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def merge_json_files(file_list, output_filename):\n",
    "    merged_data = []\n",
    "    current_id = 1\n",
    "\n",
    "    for file_path in file_list:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Cảnh báo: Không tìm thấy file {file_path}. Bỏ qua...\")\n",
    "            continue\n",
    "            \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Đảm bảo data là một danh sách\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        # Xử lý trường hợp tên trường bị swap giữa 'text' và 'content'\n",
    "                        content_text = item.get(\"text\") or item.get(\"content\") or \"\"\n",
    "                        \n",
    "                        # Tạo object mới chỉ giữ lại các trường cần thiết\n",
    "                        clean_item = {\n",
    "                            \"id\": current_id,\n",
    "                            \"content\": content_text,\n",
    "                            \"label\": item.get(\"label\", [])\n",
    "                        }\n",
    "                        merged_data.append(clean_item)\n",
    "                        current_id += 1\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Lỗi: File {file_path} không đúng định dạng JSON.\")\n",
    "\n",
    "    # Ghi ra file train.json\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"--- THÀNH CÔNG ---\")\n",
    "    print(f\"Đã gộp xong {len(merged_data)} bài vào file: {output_filename}\")\n",
    "\n",
    "# --- CẤU HÌNH ĐƯỜNG DẪN ---\n",
    "# Bạn hãy thay đổi tên file đúng với thực tế của bạn ở đây\n",
    "danh_sach_file = [\n",
    "    'data/train01_cleaned.json', \n",
    "    'data/train02_cleaned.json', \n",
    "    'data/train03_cleaned.json',\n",
    "    'data/train04_cleaned.json'\n",
    "]\n",
    "file_ket_qua = 'train.json'\n",
    "\n",
    "merge_json_files(danh_sach_file, file_ket_qua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a257c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc file train.json...\n",
      "-> Đã đọc 59 văn bản dài.\n",
      "Đang cắt nhỏ câu và tính lại vị trí nhãn...\n",
      "-> Đã lưu 368 câu ngắn vào file: train_short.json\n",
      "Đang chuyển đổi sang định dạng BIO...\n",
      "-> Đã lưu dữ liệu BIO vào file: train_bio.json\n",
      "\n",
      "=== HOÀN TẤT ===\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# --- HÀM 1: CẮT CÂU & TÍNH LẠI OFFSET ---\n",
    "def split_sentences_and_realign_labels(raw_data):\n",
    "    split_data = []\n",
    "    new_id_counter = 1\n",
    "\n",
    "    for item in raw_data:\n",
    "        original_text = item.get('text') or item.get('content')\n",
    "        original_labels = item.get('label', [])\n",
    "        \n",
    "        # Tách câu\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', original_text)\n",
    "        \n",
    "        current_cursor = 0\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if not sent: continue\n",
    "            \n",
    "            # Tìm vị trí\n",
    "            start_index = original_text.find(sent, current_cursor)\n",
    "            end_index = start_index + len(sent)\n",
    "            current_cursor = end_index\n",
    "            \n",
    "            # Tính lại Label\n",
    "            sent_labels = []\n",
    "            for lbl in original_labels:\n",
    "                l_start = lbl['start']\n",
    "                l_end = lbl['end']\n",
    "                \n",
    "                if l_start >= start_index and l_end <= end_index:\n",
    "                    new_label = {\n",
    "                        \"start\": l_start - start_index,\n",
    "                        \"end\": l_end - start_index,\n",
    "                        \"text\": lbl['text'],\n",
    "                        \"labels\": lbl['labels']\n",
    "                    }\n",
    "                    sent_labels.append(new_label)\n",
    "            \n",
    "            # --- ĐOẠN NÀY ĐÃ ĐƯỢC SỬA ---\n",
    "            # Chỉ thêm vào danh sách nếu câu có nội dung VÀ có ít nhất 1 label\n",
    "            if sent and sent_labels:  # <--- QUAN TRỌNG: Lọc bỏ câu không có label\n",
    "                new_entry = {\n",
    "                    \"id\": new_id_counter,\n",
    "                    \"original_id\": item.get('id'),\n",
    "                    \"text\": sent,\n",
    "                    \"label\": sent_labels\n",
    "                }\n",
    "                split_data.append(new_entry)\n",
    "                new_id_counter += 1\n",
    "                \n",
    "    return split_data\n",
    "\n",
    "# --- HÀM 2: LÀM SẠCH & CHUYỂN BIO ---\n",
    "def convert_to_bio_with_cleaning(json_data):\n",
    "    all_sentences = []\n",
    "    for item in json_data:\n",
    "        original_text = item.get('text') or item.get('content')\n",
    "        labels = item.get('label', [])\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized_text = ViTokenizer.tokenize(original_text)\n",
    "        tokens = tokenized_text.split()\n",
    "        \n",
    "        bio_tags = []\n",
    "        for token in tokens:\n",
    "            # Clean token\n",
    "            clean_t = re.sub(r'[^\\w\\s\\d_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂÂÊÔƠƯưăâêôơư]', '', token)\n",
    "            if not clean_t: continue \n",
    "            \n",
    "            raw_word = clean_t.replace('_', ' ')\n",
    "            \n",
    "            tag = \"O\"\n",
    "            for lbl in labels:\n",
    "                lbl_text = lbl['text'].strip()\n",
    "                if raw_word in lbl_text: \n",
    "                    l_type = lbl['labels'][0]\n",
    "                    if lbl_text.startswith(raw_word):\n",
    "                        tag = f\"B-{l_type}\"\n",
    "                    else:\n",
    "                        tag = f\"I-{l_type}\"\n",
    "                    break\n",
    "            bio_tags.append((clean_t, tag))\n",
    "        all_sentences.append(bio_tags)\n",
    "    return all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a96c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Đang đọc file gốc: train.json...\n",
      "   -> Tìm thấy 59 văn bản gốc.\n",
      "2. Đang cắt câu và lọc bỏ câu không có nhãn...\n",
      "   -> Kết quả: Giữ lại được 368 câu chất lượng (có chứa thực thể).\n",
      "   -> Đã lưu file sạch vào: train_short.json\n",
      "3. Đang chuyển đổi sang định dạng BIO để train...\n",
      "   -> Đã lưu dữ liệu BIO vào: train_bio.json\n",
      "\n",
      "=== ✅ HOÀN TẤT QUY TRÌNH TIỀN XỬ LÝ ===\n"
     ]
    }
   ],
   "source": [
    "input_file = 'train.json' # Tên file gốc của bạn\n",
    "\n",
    "try:\n",
    "    # 1. Load dữ liệu gốc\n",
    "    print(f\"1. Đang đọc file gốc: {input_file}...\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        raw_long_data = json.load(f)\n",
    "    print(f\"   -> Tìm thấy {len(raw_long_data)} văn bản gốc.\")\n",
    "\n",
    "    # 2. Cắt nhỏ thành từng câu & Lọc bỏ câu rác (không label)\n",
    "    print(\"2. Đang cắt câu và lọc bỏ câu không có nhãn...\")\n",
    "    short_data = split_sentences_and_realign_labels(raw_long_data)\n",
    "    \n",
    "    if len(short_data) == 0:\n",
    "        print(\"   ⚠️ CẢNH BÁO: Không có câu nào được giữ lại! Kiểm tra xem file gốc có label hợp lệ không.\")\n",
    "    else:\n",
    "        print(f\"   -> Kết quả: Giữ lại được {len(short_data)} câu chất lượng (có chứa thực thể).\")\n",
    "\n",
    "    # --- LƯU FILE 1: DATA ĐÃ CẮT NHỎ (Dạng JSON chuẩn) ---\n",
    "    output_short = 'train_short.json'\n",
    "    with open(output_short, 'w', encoding='utf-8') as f:\n",
    "        json.dump(short_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"   -> Đã lưu file sạch vào: {output_short}\")\n",
    "\n",
    "    # 3. Làm sạch ký tự đặc biệt & Chuyển sang BIO\n",
    "    print(\"3. Đang chuyển đổi sang định dạng BIO để train...\")\n",
    "    # Lưu ý: Cần đảm bảo bạn đã khai báo hàm convert_to_bio_with_cleaning ở trên rồi nhé\n",
    "    train_sents = convert_to_bio_with_cleaning(short_data)\n",
    "    \n",
    "    # --- LƯU FILE 2: DATA BIO ---\n",
    "    output_bio = 'train_bio.json'\n",
    "    with open(output_bio, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_sents, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"   -> Đã lưu dữ liệu BIO vào: {output_bio}\")\n",
    "    \n",
    "    print(\"\\n=== ✅ HOÀN TẤT QUY TRÌNH TIỀN XỬ LÝ ===\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Lỗi: Không tìm thấy file '{input_file}'. Hãy kiểm tra lại tên file.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Có lỗi xảy ra: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ea73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng văn bản gốc: 59\n",
      "Số lượng câu sau khi cắt: 368\n"
     ]
    }
   ],
   "source": [
    "print(f\"Số lượng văn bản gốc: {len(raw_long_data)}\")\n",
    "print(f\"Số lượng câu sau khi cắt: {len(short_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b070b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 11,\n",
       " 'original_id': 2,\n",
       " 'text': 'Lúc đó, Chủ tịch Hồ Chí Minh từ trần, vì Tố Hữu đang ốm nằm viện nên Trung ương Đảng Lao động Việt Nam không thông báo cho ông.',\n",
       " 'label': [{'start': 8,\n",
       "   'end': 28,\n",
       "   'text': 'Chủ tịch Hồ Chí Minh',\n",
       "   'labels': ['PER']},\n",
       "  {'start': 41, 'end': 47, 'text': 'Tố Hữu', 'labels': ['PER']},\n",
       "  {'start': 69,\n",
       "   'end': 102,\n",
       "   'text': 'Trung ương Đảng Lao động Việt Nam',\n",
       "   'labels': ['ORG']}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752ec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ dài trung bình 1 bài gốc: 779 ký tự\n",
      "Độ dài trung bình 1 câu mới: 124 ký tự\n"
     ]
    }
   ],
   "source": [
    "def get_avg_len(data):\n",
    "    # Use 'text' if available, otherwise 'content'. Handle empty lists to avoid ZeroDivisionError.\n",
    "    lengths = [len(item.get('text') or item.get('content', '')) for item in data]\n",
    "    if not lengths:\n",
    "        return 0\n",
    "    return sum(lengths) / len(lengths)\n",
    "\n",
    "len_old = get_avg_len(raw_long_data)\n",
    "len_new = get_avg_len(short_data)\n",
    "\n",
    "print(f\"Độ dài trung bình 1 bài gốc: {len_old:.0f} ký tự\")\n",
    "print(f\"Độ dài trung bình 1 câu mới: {len_new:.0f} ký tự\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e389f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "       BÁO CÁO XỬ LÝ DỮ LIỆU\n",
      "========================================\n",
      "1. Tổng số câu tách ra từ văn bản gốc:  368\n",
      "2. Số câu bị xóa (do không có label): - 120\n",
      "----------------------------------------\n",
      "3. SỐ CÂU CÒN LẠI (DÙNG ĐỂ TRAIN):    = 248\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# --- HÀM 1: CẮT CÂU & TÍNH LẠI OFFSET ---\n",
    "def split_sentences_and_realign_labels(raw_data):\n",
    "    split_data = []\n",
    "    new_id_counter = 1\n",
    "    \n",
    "    # --- BIẾN THỐNG KÊ ---\n",
    "    total_detected = 0   # Tổng số câu tách được từ regex\n",
    "    total_removed = 0    # Số câu bị xóa vì không có label\n",
    "    \n",
    "    for item in raw_data:\n",
    "        original_text = item.get('text') or item.get('content')\n",
    "        original_labels = item.get('label', [])\n",
    "        \n",
    "        # Tách câu\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', original_text)\n",
    "        \n",
    "        current_cursor = 0\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if not sent: continue\n",
    "            \n",
    "            total_detected += 1 # Đếm câu tìm thấy\n",
    "            \n",
    "            # Tìm vị trí\n",
    "            start_index = original_text.find(sent, current_cursor)\n",
    "            end_index = start_index + len(sent)\n",
    "            current_cursor = end_index\n",
    "            \n",
    "            # Tính lại Label\n",
    "            sent_labels = []\n",
    "            for lbl in original_labels:\n",
    "                l_start = lbl['start']\n",
    "                l_end = lbl['end']\n",
    "                \n",
    "                if l_start >= start_index and l_end <= end_index:\n",
    "                    new_label = {\n",
    "                        \"start\": l_start - start_index,\n",
    "                        \"end\": l_end - start_index,\n",
    "                        \"text\": lbl['text'],\n",
    "                        \"labels\": lbl['labels']\n",
    "                    }\n",
    "                    sent_labels.append(new_label)\n",
    "            \n",
    "            # ĐIỀU KIỆN LỌC\n",
    "            if sent and sent_labels: \n",
    "                new_entry = {\n",
    "                    \"id\": new_id_counter,\n",
    "                    \"original_id\": item.get('id'),\n",
    "                    \"text\": sent,\n",
    "                    \"label\": sent_labels\n",
    "                }\n",
    "                split_data.append(new_entry)\n",
    "                new_id_counter += 1\n",
    "            else:\n",
    "                total_removed += 1 # Tăng biến đếm nếu bị xóa\n",
    "                \n",
    "    return split_data\n",
    "\n",
    "# --- HÀM 2: LÀM SẠCH & CHUYỂN BIO ---\n",
    "def convert_to_bio_with_cleaning(json_data):\n",
    "    all_sentences = []\n",
    "    for item in json_data:\n",
    "        original_text = item.get('text') or item.get('content')\n",
    "        labels = item.get('label', [])\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized_text = ViTokenizer.tokenize(original_text)\n",
    "        tokens = tokenized_text.split()\n",
    "        \n",
    "        bio_tags = []\n",
    "        for token in tokens:\n",
    "            # Clean token\n",
    "            clean_t = re.sub(r'[^\\w\\s\\d_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂÂÊÔƠƯưăâêôơư]', '', token)\n",
    "            if not clean_t: continue \n",
    "            \n",
    "            raw_word = clean_t.replace('_', ' ')\n",
    "            \n",
    "            tag = \"O\"\n",
    "            for lbl in labels:\n",
    "                lbl_text = lbl['text'].strip()\n",
    "                if raw_word in lbl_text: \n",
    "                    l_type = lbl['labels'][0]\n",
    "                    if lbl_text.startswith(raw_word):\n",
    "                        tag = f\"B-{l_type}\"\n",
    "                    else:\n",
    "                        tag = f\"I-{l_type}\"\n",
    "                    break\n",
    "            bio_tags.append((clean_t, tag))\n",
    "        all_sentences.append(bio_tags)\n",
    "    return all_sentences\n",
    "\n",
    "# =======================================================\n",
    "# PHẦN THỰC THI CHÍNH (MAIN)\n",
    "# =======================================================\n",
    "input_file = 'train.json' \n",
    "\n",
    "try:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        raw_long_data = json.load(f)\n",
    "    \n",
    "    # Gọi hàm để xem kết quả số lượng\n",
    "    short_data = split_sentences_and_realign_labels(raw_long_data)\n",
    "\n",
    "    # Lưu file kết quả\n",
    "    with open('train_short.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(short_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Lỗi: Không tìm thấy file {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab49eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
