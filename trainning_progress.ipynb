{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99217212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc dữ liệu từ train_bio.json...\n",
      "-> Đã load thành công 248 câu.\n",
      "\n",
      "--- ĐANG TRAIN MÔ HÌNH TRUYỀN THỐNG ---\n",
      "✅ CRF: Xong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logistic Regression: Xong\n",
      "✅ Random Forest: Xong\n",
      "\n",
      "--- ĐANG TRAIN DEEP LEARNING (Bi-LSTM) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - accuracy: 0.7589 - loss: 2.3165\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8776 - loss: 0.9189\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8776 - loss: 0.6933\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8776 - loss: 0.6397\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8776 - loss: 0.5981\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8776 - loss: 0.5921\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8776 - loss: 0.5793\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8776 - loss: 0.5722\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8776 - loss: 0.5650\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8776 - loss: 0.5582\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8776 - loss: 0.5506\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8776 - loss: 0.5434\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8776 - loss: 0.5354\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8776 - loss: 0.5266\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8776 - loss: 0.5179\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.8776 - loss: 0.5087\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.8777 - loss: 0.4991\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8778 - loss: 0.4897\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8778 - loss: 0.4798\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8779 - loss: 0.4700\n",
      "✅ Bi-LSTM: Xong\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. LOAD DỮ LIỆU TỪ FILE BIO\n",
    "# ==============================================================================\n",
    "input_file = 'train_bio.json'\n",
    "\n",
    "print(f\"Đang đọc dữ liệu từ {input_file}...\")\n",
    "try:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        train_sents = json.load(f)\n",
    "    print(f\"-> Đã load thành công {len(train_sents)} câu.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Lỗi: Không tìm thấy file train_bio.json. Hãy đảm bảo bạn đã chạy bước trước đó.\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FEATURE ENGINEERING (TẠO ĐẶC TRƯNG)\n",
    "# ==============================================================================\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.has_underscore': '_' in word,\n",
    "    }\n",
    "    # Ngữ cảnh: Từ trước\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    # Ngữ cảnh: Từ sau\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "# Chuẩn bị dữ liệu cho ML truyền thống\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. HUẤN LUYỆN 3 MÔ HÌNH TRUYỀN THỐNG\n",
    "# ==============================================================================\n",
    "print(\"\\n--- ĐANG TRAIN MÔ HÌNH TRUYỀN THỐNG ---\")\n",
    "\n",
    "# 1. CRF\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100)\n",
    "crf.fit(X_train, y_train)\n",
    "print(\"✅ CRF: Xong\")\n",
    "\n",
    "# Chuẩn bị dữ liệu phẳng cho LR và RF\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "X_flat = [item for sublist in X_train for item in sublist]\n",
    "y_flat = [item for sublist in y_train for item in sublist]\n",
    "# Fit vectorizer 1 lần duy nhất với dữ liệu train\n",
    "X_flat_vec = dict_vectorizer.fit_transform(X_flat)\n",
    "\n",
    "# 2. Logistic Regression\n",
    "lr = LogisticRegression(max_iter=500, multi_class='ovr') # Dùng OvR cho nhanh\n",
    "lr.fit(X_flat_vec, y_flat)\n",
    "print(\"✅ Logistic Regression: Xong\")\n",
    "\n",
    "# 3. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=-1) # n_jobs=-1 để chạy đa luồng\n",
    "rf.fit(X_flat_vec, y_flat)\n",
    "print(\"✅ Random Forest: Xong\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. HUẤN LUYỆN DEEP LEARNING (Bi-LSTM)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- ĐANG TRAIN DEEP LEARNING (Bi-LSTM) ---\")\n",
    "\n",
    "# Tạo từ điển (Vocabulary & Tags)\n",
    "words = list(set([t[0] for sent in train_sents for t in sent]))\n",
    "tags = list(set([t[1] for sent in train_sents for t in sent]))\n",
    "\n",
    "# Thêm token đặc biệt cho padding và từ lạ (UNK)\n",
    "if \"UNK\" not in words: words.append(\"UNK\")\n",
    "if \"PAD\" not in words: words.append(\"PAD\") # Padding word\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "# Config độ dài\n",
    "MAX_LEN = 50 \n",
    "n_words = len(words)\n",
    "n_tags = len(tags)\n",
    "\n",
    "# Padding dữ liệu\n",
    "X_dl = [[word2idx.get(w[0], word2idx[\"UNK\"]) for w in s] for s in train_sents]\n",
    "X_dl = pad_sequences(X_dl, maxlen=MAX_LEN, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "y_dl = [[tag2idx[w[1]] for w in s] for s in train_sents]\n",
    "y_dl = pad_sequences(y_dl, maxlen=MAX_LEN, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y_dl = [to_categorical(i, num_classes=n_tags) for i in y_dl]\n",
    "\n",
    "# Xây dựng Model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=n_words, output_dim=50, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(units=64, return_sequences=True)),\n",
    "    TimeDistributed(Dense(n_tags, activation=\"softmax\"))\n",
    "])\n",
    "model_lstm.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train (Epoch ít để demo nhanh, thực tế nên để 20-50)\n",
    "model_lstm.fit(X_dl, np.array(y_dl), batch_size=32, epochs=20, verbose=1)\n",
    "print(\"✅ Bi-LSTM: Xong\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. HÀM DỰ ĐOÁN & TEST VỚI 3 CÂU MẪU\n",
    "# ==============================================================================\n",
    "def predict_and_compare(sentences):\n",
    "    results = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Tiền xử lý (Giống hệt lúc làm sạch data)\n",
    "        clean_sent = re.sub(r'[^\\w\\s\\d_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂÂÊÔƠƯưăâêôơư]', ' ', sent)\n",
    "        clean_sent = re.sub(r'\\s+', ' ', clean_sent).strip()\n",
    "        tokens = ViTokenizer.tokenize(clean_sent).split()\n",
    "        \n",
    "        # 1. Predict CRF\n",
    "        features = [word2features([(t, '') for t in tokens], i) for i in range(len(tokens))]\n",
    "        pred_crf = crf.predict_single(features)\n",
    "        \n",
    "        # 2. Predict LR & RF\n",
    "        # Quan trọng: Dùng .transform() chứ không fit lại\n",
    "        vec_features = dict_vectorizer.transform(features) \n",
    "        pred_lr = lr.predict(vec_features)\n",
    "        pred_rf = rf.predict(vec_features)\n",
    "        \n",
    "        # 3. Predict Bi-LSTM\n",
    "        dl_input = [word2idx.get(t, word2idx[\"UNK\"]) for t in tokens]\n",
    "        dl_input_padded = pad_sequences([dl_input], maxlen=MAX_LEN, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "        \n",
    "        pred_prob = model_lstm.predict(dl_input_padded, verbose=0)\n",
    "        pred_idx = np.argmax(pred_prob, axis=-1)[0]\n",
    "        \n",
    "        # Cắt bỏ padding để lấy đúng độ dài câu\n",
    "        pred_dl = [idx2tag[i] for i in pred_idx][:len(tokens)]\n",
    "        \n",
    "        # Lưu kết quả từng từ\n",
    "        for i, t in enumerate(tokens):\n",
    "            results.append({\n",
    "                \"Câu\": sent[:30] + \"...\", # Chỉ lấy đoạn đầu làm ID\n",
    "                \"Token\": t,\n",
    "                \"CRF\": pred_crf[i],\n",
    "                \"LogReg\": pred_lr[i],\n",
    "                \"RandForest\": pred_rf[i],\n",
    "                \"Bi-LSTM\": pred_dl[i]\n",
    "            })\n",
    "        \n",
    "        results.append({\"Câu\": \"---\", \"Token\": \"---\", \"CRF\": \"---\", \"LogReg\": \"---\", \"RandForest\": \"---\", \"Bi-LSTM\": \"---\"})\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3bcc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KẾT QUẢ DỰ ĐOÁN ===\n",
      "                              Câu           Token           CRF        LogReg    RandForest Bi-LSTM\n",
      "Tác phẩm \"Cho tôi xin một vé đ... Nguyễn_Nhật_Ánh         B-PER         B-PER         B-PER       O\n",
      "Tác phẩm \"Cho tôi xin một vé đ...             năm B-TIME / DATE             O   B-TIME/DATE       O\n",
      "Tác phẩm \"Cho tôi xin một vé đ...            2008 I-TIME / DATE I-TIME / DATE I-TIME / DATE       O\n",
      "                              ---             ---           ---           ---           ---     ---\n",
      "Chí Phèo và Thị Nở là hai nhân...        Chí_Phèo        B-WORK        B-WORK        B-WORK       O\n",
      "Chí Phèo và Thị Nở là hai nhân...              và        I-WORK             O             O       O\n",
      "Chí Phèo và Thị Nở là hai nhân...          Thị_Nở        I-WORK        B-WORK        I-WORK       O\n",
      "Chí Phèo và Thị Nở là hai nhân...         nhà_văn             O             O         B-PER       O\n",
      "Chí Phèo và Thị Nở là hai nhân...         Nam_Cao         B-PER         B-PER         B-PER       O\n",
      "                              ---             ---           ---           ---           ---     ---\n",
      "Bình Ngô đại cáo do Nguyễn Trã...            Bình        B-WORK        B-WORK        B-WORK       O\n",
      "Bình Ngô đại cáo do Nguyễn Trã...             Ngô        I-WORK        I-WORK        I-WORK       O\n",
      "Bình Ngô đại cáo do Nguyễn Trã...         đại_cáo        I-WORK        I-WORK        I-WORK       O\n",
      "Bình Ngô đại cáo do Nguyễn Trã...     Nguyễn_Trãi         B-PER         B-PER         B-PER       O\n",
      "                              ---             ---           ---           ---           ---     ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- DANH SÁCH 3 CÂU TEST ---\n",
    "test_sentences = [\n",
    "    'Tác phẩm \"Cho tôi xin một vé đi tuổi thơ\" của Nguyễn Nhật Ánh ra mắt năm 2008.',\n",
    "    'Chí Phèo và Thị Nở là hai nhân vật kinh điển của nhà văn Nam Cao.',\n",
    "    'Bình Ngô đại cáo do Nguyễn Trãi soạn thảo để tuyên cáo chiến thắng quân Minh.'\n",
    "]\n",
    "\n",
    "print(\"\\n=== KẾT QUẢ DỰ ĐOÁN ===\")\n",
    "df_result = predict_and_compare(test_sentences)\n",
    "\n",
    "# Hiển thị đẹp hơn: Chỉ hiện những dòng có ít nhất 1 mô hình tìm ra thực thể (khác O) hoặc dấu gạch ngăn cách\n",
    "mask = (df_result['CRF'] != 'O') | (df_result['LogReg'] != 'O') | (df_result['RandForest'] != 'O') | (df_result['Bi-LSTM'] != 'O') | (df_result['Token'] == '---')\n",
    "print(df_result[mask].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
