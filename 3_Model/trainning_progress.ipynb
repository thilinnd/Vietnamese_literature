{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3013fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pyvi import ViTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99217212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. LOADING DATA ---\n",
      "T·ªïng s·ªë c√¢u: 1136\n",
      "Train size: 908 (80%)\n",
      "Val size:   114 (10%)\n",
      "Test size:  114 (10%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 1. LOAD D·ªÆ LI·ªÜU & CHIA T·∫¨P (80 - 10 - 10)\n",
    "# ==============================================================================\n",
    "input_file = 'train_bio.json'\n",
    "\n",
    "print(f\"--- 1. LOADING DATA ---\")\n",
    "try:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        all_sents = json.load(f)\n",
    "    print(f\"T·ªïng s·ªë c√¢u: {len(all_sents)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y {input_file}\")\n",
    "    exit()\n",
    "\n",
    "# Chia Train (80%) v√† Temp (20%)\n",
    "train_sents, temp_sents = train_test_split(all_sents, test_size=0.2, random_state=42)\n",
    "# Chia Temp th√†nh Val (10% t·ªïng) v√† Test (10% t·ªïng) -> Chia ƒë√¥i temp_sents\n",
    "val_sents, test_sents = train_test_split(temp_sents, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_sents)} (80%)\")\n",
    "print(f\"Val size:   {len(val_sents)} (10%)\")\n",
    "print(f\"Test size:  {len(test_sents)} (10%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc26aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b31d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. PREPARING FEATURES ---\n",
      "Saved: crf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 2. FEATURE ENGINEERING (CHO CLASSICAL ML)\n",
    "# ==============================================================================\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.has_underscore': '_' in word,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({'-1:word.lower()': word1.lower(), '-1:word.istitle()': word1.istitle()})\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({'+1:word.lower()': word1.lower(), '+1:word.istitle()': word1.istitle()})\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "print(\"\\n--- 2. PREPARING FEATURES ---\")\n",
    "# T·∫°o d·ªØ li·ªáu cho c√°c m√¥ h√¨nh truy·ªÅn th·ªëng\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "# L∆∞u √Ω: LR v√† RF kh√¥ng d√πng chu·ªói, n√™n ta kh√¥ng c·∫ßn quan t√¢m th·ª© t·ª± c√¢u trong validation cho vi·ªác training c∆° b·∫£n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf4703f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. TRAINING CRF ---\n",
      ">>> REPORT CRF (tr√™n t·∫≠p Test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-CHAR     0.8684    0.8462    0.8571        39\n",
      "        B-LOC     0.6667    0.6479    0.6571        71\n",
      "        B-ORG     0.7846    0.7083    0.7445        72\n",
      "        B-PER     0.8169    0.8529    0.8345        68\n",
      "B-TIME / DATE     0.3704    0.5882    0.4545        17\n",
      "  B-TIME/DATE     0.6000    0.5085    0.5505        59\n",
      "       B-WORK     0.5833    0.2333    0.3333        30\n",
      "       I-CHAR     0.9286    0.5000    0.6500        26\n",
      "        I-LOC     0.4667    0.3889    0.4242        18\n",
      "        I-ORG     0.8480    0.7737    0.8092       137\n",
      "        I-PER     0.8000    0.9524    0.8696        21\n",
      "I-TIME / DATE     0.3333    0.4444    0.3810         9\n",
      "  I-TIME/DATE     0.7215    0.6786    0.6994        84\n",
      "       I-WORK     0.6389    0.3108    0.4182        74\n",
      "            O     0.9178    0.9658    0.9412      1665\n",
      "\n",
      "     accuracy                         0.8674      2390\n",
      "    macro avg     0.6897    0.6267    0.6416      2390\n",
      " weighted avg     0.8607    0.8674    0.8596      2390\n",
      "\n",
      "Saved: crf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 3. HU·∫§N LUY·ªÜN & ƒê√ÅNH GI√Å: CRF\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. TRAINING CRF ---\")\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100)\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "print(\">>> REPORT CRF (tr√™n t·∫≠p Test):\")\n",
    "y_pred_crf = crf.predict(X_test)\n",
    "print(flat_classification_report(y_test, y_pred_crf, digits=4))\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "with open(os.path.join(save_dir, 'crf_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(crf, f)\n",
    "print(\"Saved: crf_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "992f1537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. TRAINING LR & RF ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> REPORT Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-CHAR     0.8056    0.7436    0.7733        39\n",
      "        B-LOC     0.7015    0.6620    0.6812        71\n",
      "        B-ORG     0.7500    0.6250    0.6818        72\n",
      "        B-PER     0.8108    0.8824    0.8451        68\n",
      "B-TIME / DATE     0.3846    0.2941    0.3333        17\n",
      "  B-TIME/DATE     0.4783    0.3729    0.4190        59\n",
      "       B-WORK     0.7273    0.2667    0.3902        30\n",
      "       I-CHAR     0.6429    0.3462    0.4500        26\n",
      "        I-LOC     0.6250    0.2778    0.3846        18\n",
      "        I-ORG     0.8352    0.5547    0.6667       137\n",
      "        I-PER     0.7407    0.9524    0.8333        21\n",
      "I-TIME / DATE     0.6667    0.2222    0.3333         9\n",
      "  I-TIME/DATE     0.6667    0.6190    0.6420        84\n",
      "       I-WORK     0.8000    0.1622    0.2697        74\n",
      "            O     0.8798    0.9760    0.9254      1665\n",
      "\n",
      "     accuracy                         0.8439      2390\n",
      "    macro avg     0.7010    0.5305    0.5753      2390\n",
      " weighted avg     0.8330    0.8439    0.8252      2390\n",
      "\n",
      ">>> REPORT Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-CHAR     0.8286    0.7436    0.7838        39\n",
      "        B-LOC     0.6765    0.6479    0.6619        71\n",
      "        B-ORG     0.8302    0.6111    0.7040        72\n",
      "        B-PER     0.7317    0.8824    0.8000        68\n",
      "B-TIME / DATE     0.4400    0.6471    0.5238        17\n",
      "  B-TIME/DATE     0.5484    0.2881    0.3778        59\n",
      "       B-WORK     0.6667    0.2667    0.3810        30\n",
      "       I-CHAR     0.5714    0.4615    0.5106        26\n",
      "       I-DATE     0.0000    0.0000    0.0000         0\n",
      "        I-LOC     0.4545    0.2778    0.3448        18\n",
      "        I-ORG     0.8247    0.5839    0.6838       137\n",
      "        I-PER     0.6250    0.9524    0.7547        21\n",
      "I-TIME / DATE     0.2857    0.4444    0.3478         9\n",
      "  I-TIME/DATE     0.6957    0.5714    0.6275        84\n",
      "       I-WORK     0.8611    0.4189    0.5636        74\n",
      "            O     0.9023    0.9760    0.9377      1665\n",
      "\n",
      "     accuracy                         0.8536      2390\n",
      "    macro avg     0.6214    0.5483    0.5627      2390\n",
      " weighted avg     0.8476    0.8536    0.8423      2390\n",
      "\n",
      "Saved: vectorizer.pkl\n",
      "Saved: lr_model.pkl\n",
      "Saved: rf_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. HU·∫§N LUY·ªÜN & ƒê√ÅNH GI√Å: LOGISTIC REGRESSION & RANDOM FOREST\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 4. TRAINING LR & RF ---\")\n",
    "# Flatten d·ªØ li·ªáu (Chuy·ªÉn t·ª´ list of lists sang list ph·∫≥ng)\n",
    "X_train_flat = [item for sublist in X_train for item in sublist]\n",
    "y_train_flat = [item for sublist in y_train for item in sublist]\n",
    "\n",
    "X_test_flat = [item for sublist in X_test for item in sublist]\n",
    "y_test_flat = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "# Vector h√≥a\n",
    "v = DictVectorizer(sparse=False)\n",
    "X_train_vec = v.fit_transform(X_train_flat)\n",
    "X_test_vec = v.transform(X_test_flat) # Ch·ªâ transform, kh√¥ng fit l·∫°i\n",
    "\n",
    "# --- Logistic Regression ---\n",
    "lr = LogisticRegression(max_iter=500, multi_class='ovr', n_jobs=-1)\n",
    "lr.fit(X_train_vec, y_train_flat)\n",
    "print(\">>> REPORT Logistic Regression:\")\n",
    "print(classification_report(y_test_flat, lr.predict(X_test_vec), digits=4))\n",
    "\n",
    "# --- Random Forest ---\n",
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train_vec, y_train_flat)\n",
    "print(\">>> REPORT Random Forest:\")\n",
    "print(classification_report(y_test_flat, rf.predict(X_test_vec), digits=4))\n",
    "\n",
    "with open(os.path.join(save_dir, 'vectorizer.pkl'), 'wb') as f:\n",
    "    pickle.dump(v, f)\n",
    "print(\"Saved: vectorizer.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'lr_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(lr, f)\n",
    "print(\"Saved: lr_model.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'rf_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "print(\"Saved: rf_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35f13184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. TRAINING BI-LSTM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - accuracy: 0.8380 - loss: 1.2233 - val_accuracy: 0.8768 - val_loss: 0.5889\n",
      "Epoch 2/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.8706 - loss: 0.5930 - val_accuracy: 0.8768 - val_loss: 0.5416\n",
      "Epoch 3/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.8706 - loss: 0.5614 - val_accuracy: 0.8768 - val_loss: 0.5216\n",
      "Epoch 4/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8706 - loss: 0.5298 - val_accuracy: 0.8768 - val_loss: 0.4932\n",
      "Epoch 5/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.8720 - loss: 0.4939 - val_accuracy: 0.8791 - val_loss: 0.4613\n",
      "Epoch 6/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8779 - loss: 0.4552 - val_accuracy: 0.8851 - val_loss: 0.4329\n",
      "Epoch 7/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.8857 - loss: 0.4175 - val_accuracy: 0.8912 - val_loss: 0.4071\n",
      "Epoch 8/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.8928 - loss: 0.3830 - val_accuracy: 0.8972 - val_loss: 0.3814\n",
      "Epoch 9/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8996 - loss: 0.3504 - val_accuracy: 0.8982 - val_loss: 0.3817\n",
      "Epoch 10/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9060 - loss: 0.3232 - val_accuracy: 0.9079 - val_loss: 0.3470\n",
      "Epoch 11/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.9122 - loss: 0.3015 - val_accuracy: 0.9098 - val_loss: 0.3331\n",
      "Epoch 12/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9195 - loss: 0.2773 - val_accuracy: 0.9137 - val_loss: 0.3295\n",
      "Epoch 13/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.9253 - loss: 0.2550 - val_accuracy: 0.9188 - val_loss: 0.3186\n",
      "Epoch 14/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.9313 - loss: 0.2351 - val_accuracy: 0.9242 - val_loss: 0.2995\n",
      "Epoch 15/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.9361 - loss: 0.2192 - val_accuracy: 0.9268 - val_loss: 0.2930\n",
      "Epoch 16/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9399 - loss: 0.2046 - val_accuracy: 0.9281 - val_loss: 0.2892\n",
      "Epoch 17/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9435 - loss: 0.1892 - val_accuracy: 0.9275 - val_loss: 0.2929\n",
      "Epoch 18/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9472 - loss: 0.1772 - val_accuracy: 0.9270 - val_loss: 0.3014\n",
      "Epoch 19/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9513 - loss: 0.1636 - val_accuracy: 0.9260 - val_loss: 0.2986\n",
      "Epoch 20/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9519 - loss: 0.1567 - val_accuracy: 0.9272 - val_loss: 0.3155\n",
      "Epoch 21/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9561 - loss: 0.1482 - val_accuracy: 0.9282 - val_loss: 0.2966\n",
      "Epoch 22/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.9582 - loss: 0.1388 - val_accuracy: 0.9293 - val_loss: 0.3086\n",
      "Epoch 23/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9616 - loss: 0.1287 - val_accuracy: 0.9289 - val_loss: 0.2951\n",
      "Epoch 24/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9625 - loss: 0.1234 - val_accuracy: 0.9291 - val_loss: 0.3056\n",
      "Epoch 25/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9641 - loss: 0.1170 - val_accuracy: 0.9284 - val_loss: 0.2914\n",
      "Epoch 26/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9658 - loss: 0.1110 - val_accuracy: 0.9296 - val_loss: 0.3021\n",
      "Epoch 27/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9678 - loss: 0.1056 - val_accuracy: 0.9311 - val_loss: 0.2941\n",
      "Epoch 28/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9696 - loss: 0.0994 - val_accuracy: 0.9304 - val_loss: 0.3247\n",
      "Epoch 29/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9717 - loss: 0.0940 - val_accuracy: 0.9293 - val_loss: 0.3286\n",
      "Epoch 30/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.9728 - loss: 0.0896 - val_accuracy: 0.9281 - val_loss: 0.3444\n",
      "Epoch 31/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.9742 - loss: 0.0854 - val_accuracy: 0.9293 - val_loss: 0.3374\n",
      "Epoch 32/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.9751 - loss: 0.0809 - val_accuracy: 0.9305 - val_loss: 0.3442\n",
      "Epoch 33/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9762 - loss: 0.0778 - val_accuracy: 0.9302 - val_loss: 0.3093\n",
      "Epoch 34/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9770 - loss: 0.0753 - val_accuracy: 0.9312 - val_loss: 0.3444\n",
      "Epoch 35/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9793 - loss: 0.0701 - val_accuracy: 0.9312 - val_loss: 0.3601\n",
      "Epoch 36/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.9798 - loss: 0.0662 - val_accuracy: 0.9305 - val_loss: 0.3328\n",
      "Epoch 37/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9799 - loss: 0.0649 - val_accuracy: 0.9318 - val_loss: 0.3591\n",
      "Epoch 38/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9809 - loss: 0.0611 - val_accuracy: 0.9319 - val_loss: 0.3385\n",
      "Epoch 39/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9824 - loss: 0.0584 - val_accuracy: 0.9305 - val_loss: 0.3553\n",
      "Epoch 40/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.9829 - loss: 0.0557 - val_accuracy: 0.9312 - val_loss: 0.3486\n",
      "Epoch 41/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.9841 - loss: 0.0536 - val_accuracy: 0.9312 - val_loss: 0.3782\n",
      "Epoch 42/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9842 - loss: 0.0518 - val_accuracy: 0.9298 - val_loss: 0.3375\n",
      "Epoch 43/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.9854 - loss: 0.0494 - val_accuracy: 0.9325 - val_loss: 0.3607\n",
      "Epoch 44/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9861 - loss: 0.0474 - val_accuracy: 0.9323 - val_loss: 0.3736\n",
      "Epoch 45/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9857 - loss: 0.0461 - val_accuracy: 0.9318 - val_loss: 0.3796\n",
      "Epoch 46/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9867 - loss: 0.0448 - val_accuracy: 0.9335 - val_loss: 0.3812\n",
      "Epoch 47/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.9871 - loss: 0.0425 - val_accuracy: 0.9314 - val_loss: 0.3737\n",
      "Epoch 48/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9876 - loss: 0.0420 - val_accuracy: 0.9321 - val_loss: 0.3785\n",
      "Epoch 49/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9877 - loss: 0.0405 - val_accuracy: 0.9335 - val_loss: 0.3925\n",
      "Epoch 50/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9877 - loss: 0.0409 - val_accuracy: 0.9307 - val_loss: 0.4090\n",
      ">>> REPORT Bi-LSTM:\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-CHAR     0.7500    0.8108    0.7792        37\n",
      "       B-DATE     0.0000    0.0000    0.0000         0\n",
      "        B-LOC     0.5469    0.4930    0.5185        71\n",
      "        B-ORG     0.7931    0.6389    0.7077        72\n",
      "        B-PER     0.8103    0.7015    0.7520        67\n",
      "B-TIME / DATE     0.3000    0.5294    0.3830        17\n",
      "  B-TIME/DATE     0.4884    0.3559    0.4118        59\n",
      "       B-WORK     0.5238    0.3667    0.4314        30\n",
      "       I-CHAR     0.5000    0.3913    0.4390        23\n",
      "        I-LOC     0.5714    0.2222    0.3200        18\n",
      "        I-ORG     0.8396    0.6496    0.7325       137\n",
      "        I-PER     0.6429    0.9000    0.7500        20\n",
      "I-TIME / DATE     0.1429    0.1111    0.1250         9\n",
      "  I-TIME/DATE     0.7018    0.4762    0.5674        84\n",
      "       I-WORK     0.6000    0.4459    0.5116        74\n",
      "            O     0.8919    0.9602    0.9248      1633\n",
      "\n",
      "     accuracy                         0.8341      2351\n",
      "    macro avg     0.5689    0.5033    0.5221      2351\n",
      " weighted avg     0.8245    0.8341    0.8247      2351\n",
      "\n",
      "Saved: bi_lstm_config.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\vthuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: bi_lstm_model.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. HU·∫§N LUY·ªÜN & ƒê√ÅNH GI√Å: BI-LSTM\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 5. TRAINING BI-LSTM ---\")\n",
    "\n",
    "# T·∫°o t·ª´ ƒëi·ªÉn t·ª´ TO√ÄN B·ªò d·ªØ li·ªáu ƒë·ªÉ tr√°nh l·ªói out-of-vocab\n",
    "words = list(set([t[0] for sent in all_sents for t in sent]))\n",
    "tags = list(set([t[1] for sent in all_sents for t in sent]))\n",
    "\n",
    "if \"UNK\" not in words: words.append(\"UNK\")\n",
    "if \"PAD\" not in words: words.append(\"PAD\")\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "MAX_LEN = 50\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)\n",
    "\n",
    "# H√†m helper ƒë·ªÉ encode d·ªØ li·ªáu cho DL\n",
    "def encode_data(sents, word2idx, tag2idx, max_len):\n",
    "    X = [[word2idx.get(w[0], word2idx[\"UNK\"]) for w in s] for s in sents]\n",
    "    X = pad_sequences(X, maxlen=max_len, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "    \n",
    "    y = [[tag2idx[w[1]] for w in s] for s in sents]\n",
    "    y = pad_sequences(y, maxlen=max_len, padding=\"post\", value=tag2idx[\"O\"]) # Pad label b·∫±ng 'O'\n",
    "    y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "    return X, np.array(y)\n",
    "\n",
    "# Encode c√°c t·∫≠p\n",
    "X_train_dl, y_train_dl = encode_data(train_sents, word2idx, tag2idx, MAX_LEN)\n",
    "X_val_dl, y_val_dl = encode_data(val_sents, word2idx, tag2idx, MAX_LEN) # D√πng Validation ·ªü ƒë√¢y\n",
    "X_test_dl, y_test_dl = encode_data(test_sents, word2idx, tag2idx, MAX_LEN)\n",
    "\n",
    "# Build Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=n_words, output_dim=50, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(units=64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    TimeDistributed(Dense(n_tags, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train c√≥ s·ª≠ d·ª•ng validation_data\n",
    "history = model.fit(X_train_dl, y_train_dl, \n",
    "                    validation_data=(X_val_dl, y_val_dl), \n",
    "                    batch_size=32, epochs=50, verbose=1)\n",
    "\n",
    "# ƒê√°nh gi√° Bi-LSTM\n",
    "print(\">>> REPORT Bi-LSTM:\")\n",
    "y_pred_probs = model.predict(X_test_dl)\n",
    "y_pred_indices = np.argmax(y_pred_probs, axis=-1)\n",
    "y_test_indices = np.argmax(y_test_dl, axis=-1)\n",
    "\n",
    "# Chuy·ªÉn ng∆∞·ª£c t·ª´ s·ªë v·ªÅ nh√£n (b·ªè padding ƒë·ªÉ t√≠nh ch√≠nh x√°c)\n",
    "pred_tags_flat = []\n",
    "true_tags_flat = []\n",
    "\n",
    "for i in range(len(test_sents)):\n",
    "    # L·∫•y ƒë·ªô d√†i th·ª±c t·∫ø c·ªßa c√¢u (tr√°nh l·∫•y ph·∫ßn padding)\n",
    "    true_len = len(test_sents[i])\n",
    "    # Ch·ªâ l·∫•y ph·∫ßn d·ª± ƒëo√°n t∆∞∆°ng ·ª©ng v·ªõi ƒë·ªô d√†i c√¢u\n",
    "    p_tags = [idx2tag[idx] for idx in y_pred_indices[i][:true_len]]\n",
    "    t_tags = [idx2tag[idx] for idx in y_test_indices[i][:true_len]]\n",
    "    \n",
    "    pred_tags_flat.extend(p_tags)\n",
    "    true_tags_flat.extend(t_tags)\n",
    "\n",
    "print(classification_report(true_tags_flat, pred_tags_flat, digits=4))\n",
    "\n",
    "bi_lstm_config = {\n",
    "    'word2idx': word2idx,\n",
    "    'tag2idx': tag2idx,\n",
    "    'idx2tag': idx2tag,\n",
    "    'max_len': MAX_LEN\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, 'bi_lstm_config.pkl'), 'wb') as f:\n",
    "    pickle.dump(bi_lstm_config, f)\n",
    "print(\"Saved: bi_lstm_config.pkl\")\n",
    "\n",
    "model.save(os.path.join(save_dir, 'bi_lstm_model.keras'))\n",
    "print(\"Saved: bi_lstm_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3bcc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Models ---\n",
      ">>> Ready to predict!\n",
      "\n",
      "================ PREDICTION REPORT ================\n",
      "\n",
      "üìù Input Raw: T√°c ph·∫©m \"Cho t√¥i xin m·ªôt v√© ƒëi tu·ªïi th∆°\" c·ªßa Nguy·ªÖn Nh·∫≠t √Ånh ra m·∫Øt nƒÉm 2008.\n",
      "üîß Cleaned Tokens: ['T√°c_ph·∫©m', 'Cho', 't√¥i', 'xin', 'm·ªôt', 'v√©', 'ƒëi', 'tu·ªïi_th∆°', 'c·ªßa', 'Nguy·ªÖn_Nh·∫≠t_√Ånh', 'ra_m·∫Øt', 'nƒÉm', '2008']\n",
      "TOKEN                | CRF        | LogReg     | Bi-LSTM   \n",
      "------------------------------------------------------------\n",
      "T√°c_ph·∫©m             | O          | O          | O         \n",
      "Cho                  | O          | O          | O         \n",
      "t√¥i                  | O          | O          | O         \n",
      "xin                  | O          | O          | O         \n",
      "m·ªôt                  | O          | O          | O         \n",
      "v√©                   | O          | O          | O         \n",
      "ƒëi                   | O          | O          | O         \n",
      "tu·ªïi_th∆°             | O          | O          | O         \n",
      "c·ªßa                  | O          | O          | O         \n",
      "Nguy·ªÖn_Nh·∫≠t_√Ånh      | B-PER      | B-PER      | O         \n",
      "ra_m·∫Øt               | O          | O          | O         \n",
      "nƒÉm                  | O          | O          | O         \n",
      "2008                 | B-TIME / DATE | B-TIME / DATE | B-TIME / DATE\n",
      "\n",
      "üìù Input Raw: Ch√≠ Ph√®o v√† Th·ªã N·ªü l√† hai nh√¢n v·∫≠t kinh ƒëi·ªÉn c·ªßa nh√† vƒÉn Nam Cao.\n",
      "üîß Cleaned Tokens: ['Ch√≠_Ph√®o', 'v√†', 'Th·ªã_N·ªü', 'l√†', 'hai', 'nh√¢n_v·∫≠t', 'kinh_ƒëi·ªÉn', 'c·ªßa', 'nh√†_vƒÉn', 'Nam_Cao']\n",
      "TOKEN                | CRF        | LogReg     | Bi-LSTM   \n",
      "------------------------------------------------------------\n",
      "Ch√≠_Ph√®o             | B-WORK     | O          | B-CHAR    \n",
      "v√†                   | I-WORK     | O          | O         \n",
      "Th·ªã_N·ªü               | I-WORK     | B-PER      | O         \n",
      "l√†                   | O          | O          | O         \n",
      "hai                  | O          | O          | O         \n",
      "nh√¢n_v·∫≠t             | O          | O          | O         \n",
      "kinh_ƒëi·ªÉn            | O          | O          | O         \n",
      "c·ªßa                  | O          | O          | O         \n",
      "nh√†_vƒÉn              | O          | O          | O         \n",
      "Nam_Cao              | B-PER      | B-PER      | B-PER     \n",
      "\n",
      "üìù Input Raw: B√¨nh Ng√¥ ƒë·∫°i c√°o do Nguy·ªÖn Tr√£i so·∫°n th·∫£o ƒë·ªÉ tuy√™n c√°o chi·∫øn th·∫Øng qu√¢n Minh.\n",
      "üîß Cleaned Tokens: ['B√¨nh', 'Ng√¥', 'ƒë·∫°i_c√°o', 'do', 'Nguy·ªÖn_Tr√£i', 'so·∫°n_th·∫£o', 'ƒë·ªÉ', 'tuy√™n_c√°o', 'chi·∫øn_th·∫Øng', 'qu√¢n', 'Minh']\n",
      "TOKEN                | CRF        | LogReg     | Bi-LSTM   \n",
      "------------------------------------------------------------\n",
      "B√¨nh                 | B-WORK     | B-WORK     | B-WORK    \n",
      "Ng√¥                  | I-WORK     | I-WORK     | I-WORK    \n",
      "ƒë·∫°i_c√°o              | I-WORK     | I-WORK     | I-WORK    \n",
      "do                   | O          | O          | O         \n",
      "Nguy·ªÖn_Tr√£i          | B-PER      | B-PER      | B-PER     \n",
      "so·∫°n_th·∫£o            | O          | O          | O         \n",
      "ƒë·ªÉ                   | O          | O          | O         \n",
      "tuy√™n_c√°o            | O          | O          | O         \n",
      "chi·∫øn_th·∫Øng          | O          | O          | O         \n",
      "qu√¢n                 | B-ORG      | O          | B-ORG     \n",
      "Minh                 | I-ORG      | I-ORG      | I-ORG     \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from pyvi import ViTokenizer\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. H√ÄM TI·ªÄN X·ª¨ L√ù (GI·ªêNG H·ªÜT L√öC TRAIN)\n",
    "# ==============================================================================\n",
    "def preprocess_text_for_prediction(text):\n",
    "    \"\"\"\n",
    "    M√¥ ph·ªèng l·∫°i h√†m 'convert_to_bio_with_cleaning' nh∆∞ng b·ªè ph·∫ßn g√°n nh√£n.\n",
    "    Input: C√¢u vƒÉn b·∫£n th√¥.\n",
    "    Output: List c√°c token ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch y h·ªát d·ªØ li·ªáu train.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize b·∫±ng PyVi\n",
    "    tokenized_text = ViTokenizer.tokenize(text)\n",
    "    raw_tokens = tokenized_text.split()\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    # 2. L√†m s·∫°ch b·∫±ng Regex (COPY Y H·ªÜT T·ª™ CODE C·ª¶A B·∫†N)\n",
    "    for token in raw_tokens:\n",
    "        clean_t = re.sub(r'[^\\w\\s\\d_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ√Ç√ä√î∆†∆Ø∆∞ƒÉ√¢√™√¥∆°∆∞]', '', token)\n",
    "        \n",
    "        # Ch·ªâ gi·ªØ l·∫°i token n√†o c√≤n n·ªôi dung sau khi clean\n",
    "        if clean_t:\n",
    "            cleaned_tokens.append(clean_t)\n",
    "            \n",
    "    return cleaned_tokens\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. H√ÄM T·∫†O FEATURES (GI·ªêNG L√öC TRAIN)\n",
    "# ==============================================================================\n",
    "def get_features_for_prediction(sent):\n",
    "    sent_features = []\n",
    "    for i in range(len(sent)):\n",
    "        word = sent[i]\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'word.has_underscore': '_' in word, # Feature quan tr·ªçng c·ªßa PyVi\n",
    "        }\n",
    "        if i > 0:\n",
    "            word1 = sent[i-1]\n",
    "            features.update({'-1:word.lower()': word1.lower(), '-1:word.istitle()': word1.istitle()})\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "\n",
    "        if i < len(sent)-1:\n",
    "            word1 = sent[i+1]\n",
    "            features.update({'+1:word.lower()': word1.lower(), '+1:word.istitle()': word1.istitle()})\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "        sent_features.append(features)\n",
    "    return sent_features\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. LOAD MODEL\n",
    "# ==============================================================================\n",
    "save_dir = 'saved_models' # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng\n",
    "\n",
    "print(\"--- Loading Models ---\")\n",
    "# Load CRF\n",
    "with open(os.path.join(save_dir, 'crf_model.pkl'), 'rb') as f:\n",
    "    loaded_crf = pickle.load(f)\n",
    "\n",
    "# Load Classical ML Components\n",
    "with open(os.path.join(save_dir, 'vectorizer.pkl'), 'rb') as f:\n",
    "    loaded_v = pickle.load(f)\n",
    "with open(os.path.join(save_dir, 'lr_model.pkl'), 'rb') as f:\n",
    "    loaded_lr = pickle.load(f)\n",
    "with open(os.path.join(save_dir, 'rf_model.pkl'), 'rb') as f:\n",
    "    loaded_rf = pickle.load(f)\n",
    "\n",
    "# Load Bi-LSTM Components\n",
    "with open(os.path.join(save_dir, 'bi_lstm_config.pkl'), 'rb') as f:\n",
    "    lstm_config = pickle.load(f)\n",
    "loaded_lstm = load_model(os.path.join(save_dir, 'bi_lstm_model.keras'))\n",
    "\n",
    "word2idx = lstm_config['word2idx']\n",
    "idx2tag = lstm_config['idx2tag']\n",
    "MAX_LEN = lstm_config['max_len']\n",
    "print(\">>> Ready to predict!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TH·ª∞C HI·ªÜN D·ª∞ ƒêO√ÅN\n",
    "# ==============================================================================\n",
    "\n",
    "# D·ªØ li·ªáu m·ªõi (ch∆∞a t·ª´ng th·∫•y l√∫c train)\n",
    "test_sentences = [\n",
    "    'T√°c ph·∫©m \"Cho t√¥i xin m·ªôt v√© ƒëi tu·ªïi th∆°\" c·ªßa Nguy·ªÖn Nh·∫≠t √Ånh ra m·∫Øt nƒÉm 2008.',\n",
    "    'Ch√≠ Ph√®o v√† Th·ªã N·ªü l√† hai nh√¢n v·∫≠t kinh ƒëi·ªÉn c·ªßa nh√† vƒÉn Nam Cao.',\n",
    "    'B√¨nh Ng√¥ ƒë·∫°i c√°o do Nguy·ªÖn Tr√£i so·∫°n th·∫£o ƒë·ªÉ tuy√™n c√°o chi·∫øn th·∫Øng qu√¢n Minh.'\n",
    "]\n",
    "\n",
    "print(\"\\n================ PREDICTION REPORT ================\")\n",
    "\n",
    "for text in test_sentences:\n",
    "    print(f\"\\nüìù Input Raw: {text}\")\n",
    "    \n",
    "    # B∆Ø·ªöC QUAN TR·ªåNG: D√πng h√†m preprocess chu·∫©n\n",
    "    tokens = preprocess_text_for_prediction(text)\n",
    "    print(f\"üîß Cleaned Tokens: {tokens}\") \n",
    "    \n",
    "    if not tokens:\n",
    "        print(\"-> C√¢u n√†y b·ªã l·ªçc h·∫øt s·∫°ch k√Ω t·ª± ƒë·∫∑c bi·ªát, b·ªè qua.\")\n",
    "        continue\n",
    "\n",
    "    # --- A. CRF ---\n",
    "    features = get_features_for_prediction(tokens)\n",
    "    pred_crf = loaded_crf.predict_single(features)\n",
    "    \n",
    "    # --- B. Logistic Regression & Random Forest ---\n",
    "    features_vec = loaded_v.transform(features)\n",
    "    pred_lr = loaded_lr.predict(features_vec)\n",
    "    pred_rf = loaded_rf.predict(features_vec)\n",
    "    \n",
    "    # --- C. Bi-LSTM ---\n",
    "    seq_idx = [word2idx.get(w, word2idx['UNK']) for w in tokens]\n",
    "    seq_padded = pad_sequences([seq_idx], maxlen=MAX_LEN, padding='post', value=word2idx['PAD'])\n",
    "    pred_prob_lstm = loaded_lstm.predict(seq_padded, verbose=0)\n",
    "    pred_idx_lstm = np.argmax(pred_prob_lstm, axis=-1)[0]\n",
    "    \n",
    "    # L·∫•y nh√£n t∆∞∆°ng ·ª©ng ƒë·ªô d√†i th·ª±c t·∫ø\n",
    "    pred_lstm = [idx2tag[i] for i in pred_idx_lstm[:len(tokens)]]\n",
    "\n",
    "    # --- IN K·∫æT QU·∫¢ ---\n",
    "    print(f\"{'TOKEN':<20} | {'CRF':<10} | {'LogReg':<10} | {'Bi-LSTM':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        t_crf = pred_crf[i]\n",
    "        t_lr = pred_lr[i]\n",
    "        # t_rf = pred_rf[i] # C√≥ th·ªÉ th√™m v√†o n·∫øu mu·ªën\n",
    "        t_lstm = pred_lstm[i] if i < len(pred_lstm) else \"O\"\n",
    "        \n",
    "        # Ch·ªâ in nh·ªØng token c√≥ nh√£n kh√°c 'O' ƒë·ªÉ d·ªÖ nh√¨n (ho·∫∑c in h·∫øt t√πy b·∫°n)\n",
    "        # if t_crf != 'O' or t_lstm != 'O': \n",
    "        print(f\"{tokens[i]:<20} | {t_crf:<10} | {t_lr:<10} | {t_lstm:<10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
